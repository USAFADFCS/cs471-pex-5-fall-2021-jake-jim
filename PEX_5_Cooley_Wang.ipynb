{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KBl_Syo5Fc5"
      },
      "source": [
        "## Jake Cooley\n",
        "## Jim Wang\n",
        "## Documentation: Referenced ACM Code of Ethics for the essay at https://www.acm.org/code-of-ethics for principles 1.4 and 1.7. Also referenced https://github.com/ultralytics/yolov5/issues/2696 to understand how we can count the number of assets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNElZJ2ZOEHQ"
      },
      "source": [
        "# PEX 5 YOLO!\n",
        "## 80 Points\n",
        "\n",
        "In PEX 5 you will train the YOLO object detection algorihtm to detect objects that interest you. If you are not feeling creative, you can collect data and train a model to detect playing card suits and face values. You will then do something interesting with your model. For example, with the playing card data set, you can use the web cam to play blackjack without user input. \n",
        "\n",
        "The graded objectives for PEX 5 are:\n",
        "\n",
        "(10 pts) Collect and label data for use in building a custom YOLO model\n",
        "\n",
        "(20 pts) Train the YOLO model to high accuracy for your domain\n",
        "\n",
        "  **\"Lt Col Maher, what is high accuracy?\" ... It depends on your domain. If you want to make sure you recieve full credit, write down all of your efforts to improve the accuracy of the model, and write a statement why you think you have achieved peak accuracy in this model. Charts and graphs will help to justify your stance. \n",
        "\n",
        "(10 pts) Use data analysis to interpret the Mean Average Precision, accuracy and recall of your model\n",
        "\n",
        "(20 pts) Enable object detection through a web camera\n",
        "\n",
        "(20 pts) Write code that does something interesting with your model. \n",
        "\n",
        "(10 pts) Write a 300-500 word essay describing the possible ethical implications of your project. Reference an ethical framework to justify your view. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFLMd6hqQ3q_"
      },
      "source": [
        "**AUTHORIZED RESOURCES:** Any material from the CS 471 course site and online resources. You may reuse online code as long as you describe what the code is doing in your comments and you modify it to solve this problem. Don't forget to document any online code sources. \n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "*  Never copy another person’s or group’s work and submit it as your own.\n",
        "*  Do not jointly create a program or complete this assignment unless explicitly allowed.\n",
        "*   You must document all help received from sources other than your instructor or instructor-provided course materials (including your textbook).\n",
        "\n",
        "**Documentation Policy:**\n",
        "\n",
        "*   You must document all help received from any source other than your instructor or instructor-provided materials, including your textbook (unless directly quoting or paraphrasing).\n",
        "*   The documentation statement must explicitly describe WHAT assistance was provided, WHERE on the assignment the assistance was provided, and WHO provided the assistance, and HOW it was used in completing the assignment.\n",
        "*   If no help was received on this assignment, the documentation statement must state “None.”\n",
        "*   If you checked answers with anyone, you must document with whom on which problems. You must document whether or not you made any changes, and if you did make changes you must document the problems you changed and the reasons why.\n",
        "*   Vague documentation statements must be corrected before the assignment will be graded and will result in a 5% deduction on the assignment.\n",
        "\n",
        "**Turn-in Policies:**\n",
        "\n",
        "*   On-time turn-in is at the specific day and time listed above.\n",
        "*   Post the required solution files to your Github Classroom repo.\n",
        "*   Only 1 turn-in required per team.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6p6HAuM4HGM"
      },
      "source": [
        "## (10 pts) Task 1 Train and Label data for use in building a custom YOLO model. \n",
        "I recommend creating an account with Roboflow.com to upload pictures and label the data set. There are several other tools available that you are welcome to use. If you use another tool, I will need a way to access your data; please provide that method in the text box below. If you use Roboflow, copy and paste the link to your Roboflow project page below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzQB-nUy4u5p"
      },
      "source": [
        "### Answer to Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP5cdlXO4hI0"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"smmDdaroh5TKuoP6JET8\")\n",
        "project = rf.workspace().project(\"pex5-aiabk\")\n",
        "dataset = project.version(8).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKRqdZzf4xOx"
      },
      "source": [
        "## (20 pts) Task 2 Train a Yolo model to high accuracy. \n",
        "\n",
        "Keep a log of your experimentation with improving your model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXTPCt0g5AHo"
      },
      "outputs": [],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "torch.cuda.is_available()\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul2kFpyk4p4o"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "print(torch.cuda.is_available())\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIZSi7I-4svH"
      },
      "outputs": [],
      "source": [
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMpcURc54tkS"
      },
      "outputs": [],
      "source": [
        "rf = Roboflow(api_key=\"smmDdaroh5TKuoP6JET8\")\n",
        "project = rf.workspace().project(\"pex5-aiabk\")\n",
        "dataset = project.version(8).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWoqcY9i4veD"
      },
      "outputs": [],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 1000 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache\n",
        "# place best.pt into the yolov5 directory and change yolov56s.pt to best.pt when you run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q3-gO1F5LX5"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIdzGU2o5KwP"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source {dataset.location}/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6ChprvP5NCr"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv40JgCq5WRI"
      },
      "source": [
        "## (20 pts) Task 3 Justify how well your model performs\n",
        "Provide charts showing at least the Mean Average Precision, Accuracy, and Recall of your model. Discuss the charts and what these results mean. You may want to include a discussion on overfitting and underfitting in your discussion. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBi5Qy_n5m3L"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzJlZA7v5sR3"
      },
      "source": [
        "### Answer to Task 3. \n",
        "We ran our model for 1000 epochs but colab quit after ~300 because our loss was not changing. This suggests that we need more data. \n",
        "\n",
        "As we ran our epochs, our loss values decreased significantly and seemed to converge. This indicates that our model is indeed learning.\n",
        "Further, as we continued to run our model, our mAP0.5 (mean average precision) increased and converged to about 0.23. This metric takes into account both the classification accuracy and the precision of the bounding box location. Our value here  means that our model only rarely classifies images correctly. \n",
        "\n",
        "![metrics](yoloMetrics.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys-bw08N5xVj"
      },
      "source": [
        "## (20 pts) Task 4. Connect your model to Webcam streaming\n",
        "\n",
        "Enable your model to predict in real-time on a web camera, using your custom model. If the objects you train your model to are not something commonly available, please provide the web cam code and a pre-recorded video of you detecting the object. \n",
        "\n",
        "Hint: On the left hand side, Google Colab gives you code for accessing your webcam with Javascript. Use this code to get your webcam working. YOLO has a webcam input functionality built-in, but you cannot use this functionality, because it is attempting to open the webcam on Google's server and not your laptop. To execute this step\n",
        "\n",
        "1.   Modify the webcam code so that it will open the webcam,\n",
        "2.   take a picture,\n",
        "1.   close the webcam,\n",
        "2.   run the detection algorithm,\n",
        "1.   display the detection image\n",
        "2.   and then repeat 5 times.\n",
        "\n",
        "This will not be the same as real-time webcam footage, but it will get you as close as you can get on Google Colab. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTcFxkK2kyof"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL-IakM_kzvc"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WJ5oSMvk1Su"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best_try.pt --img 416 --conf 0.1 --source /content/yolov5/photo.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypCp6QEX6IB4"
      },
      "source": [
        "###Answer to Task 4\n",
        "(Optional) Add pre-recorded video, if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfzjxmhy7jbI"
      },
      "source": [
        "##(10 pts) Task 5. Create something interesting using your model\n",
        "\n",
        "You have a limited time, so don't make this a huge feature, just something cool your model could do. For example, something that counts the objects coming across the webcam would receive full points. If you have a more creative idea, I may be inclined to add some bonus points, but make sure you are taking care of your other classwork as well. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9PN_YgI5of1"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "for exp in glob.glob('/content/yolov5/runs/detect/*'): #assuming JPG\n",
        "    #print(exp)\n",
        "    for imageName in glob.glob(exp + '/*.jpg'):\n",
        "      display(Image(filename=imageName))\n",
        "      # This is where our cool thing is\n",
        "      results = model(imageName)\n",
        "      results.print()  # or .show(), .save()\n",
        "      print(results.pandas().xyxy[0])\n",
        "      if(len(results.pandas().xyxy[0]) < 1):\n",
        "        print(\"There may be something holding your card, but I can't tell what it is\")\n",
        "      else:\n",
        "        print(\"There is a human holding your card, so this will probably be a fair game\")  \n",
        "      print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvelYJcI8Ove"
      },
      "source": [
        "### (10 pts) Task 6. Write 300-500 words on the ethical implications of your project.\n",
        "Make sure you support your thoughts with ethical frameworks from ACM, IEEE, or any other reputible source. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktf121Zv8c8c"
      },
      "source": [
        "## Answer to Task 6\n",
        "\n",
        "  Out of the many fields of computer science, artificial intelligence is a growing discipline that holds many ethical implications for both good and for ill. A major point of ethical controversy for artificial intelligence is the use of private information to train artificial intelligence agents or categorize data. For example, cellphone users may find benefits from the smart voice assistants on their phones, but probably do not want the recordings of their voice to be stored by the company providing the service. There are privacy concerns from the user because they are not sure if they can trust the company not to use the content of their voice recordings to exploit the user in the future. If the user is not actively using the voice assistant but the phone picks up audio that the user is looking to buy a new product, an ethical question is raised regarding whether or not the company can give that data to advertisers. According to the ACM Code of Ethics principle 1.7, computer professionals should honor confidentiality with client data except in cases where it is in violation of the law or the Code. In cases where the voice recording is not in violation of these and would simply be used to benefit the company, the ACM Code of Ethics advises that the user data should not be sold to an advertiser. \n",
        "  \n",
        "  In terms of our specific project, there are ethical considerations for image recognition software that uses artificial intelligence. According to the ACM Code of Ethics principle 1.4, computing professionals need to foster fair participation of all people and prejudicial discrimination is an explicit violation of the Code. If we were to apply our image recognition program to a different set of test data, identifying potential good candidates for a job interview based on applicant photos, we could run into an issue of prejudicial discrimination if the artificial intelligence algorithm finds a pattern of a specific race or nationality being selected as a “good candidate” more often than other nationalities or race of people. If a watchful eye is not kept over how the algorithm is differentiating good candidates from bad candidates, then the computing professionals are creating systems that disenfranchise or oppress other people, something that violates the ACM Code of Ethics because it creates unfair discrimination. In this case, it may even be wise to use a different measure of merit aside from candidate photos because of the inherent risk associated with unfair discrimination, so it may be better to train an artificial intelligence agent to analyze user applications or a measure of merit that is not associated with appearance instead.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PEX_5_Cooley_Wang.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
